Good morning! Here's your coding interview problem for today.

This problem was asked by Google.

Design a system to crawl and copy all of Wikipedia using a distributed network
of machines.

More specifically, suppose your server has access to a set of client machines.
Your client machines can execute code you have written to access Wikipedia
pages, download and parse their data, and write the results to a database.

Some questions you may want to consider as part of your solution are:

 * How will you reach as many pages as possible?
 * How can you keep track of pages that have already been visited?
 * How will you deal with your client machines being blacklisted?
 * How can you update your database when Wikipedia pages are added or updated?


--------------------------------------------------------------------------------

Upgrade to premium
[https://www.dailycodingproblem.com/subscribe?email=mishravishal958%40gmail.com&ref=csdojo] 
and get in-depth solutions to every problem, including this one. Since you were
referred by one of our affiliates, you'll get a 10% discount on checkout!

If you liked this problem, feel free to forward it along so they can subscribe
here [https://www.dailycodingproblem.com/]! As always, shoot us an email if
there's anything we can help with!


--------------------------------------------------------------------------------

Master algorithms together on Binary Search [https://binarysearch.io/?ref=dcp].
Create a room, invite your friends, and race to finish the problem!


--------------------------------------------------------------------------------

No more? Snooze or unsubscribe
[https://dailycodingproblem.com/unsubscribe?unsubscribeKey=c5ae6a38585c7eaa09f0d49f11f3d7c2f718630c5141670a65cddcd4a35cc0a5e9a43881]
.